{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "We'll start out with a plain and simple chat bot using OpenAI and gpt-4o-mini.  You will need an apikey for OpenAI to run this example.  \n",
    "You can use a different model if you'd like. \n",
    "\n",
    "For this demo, in addition to an `OPENAI_API_KEY`, you will need a Mindlytics api key and project id.  These can be sent in environment variables; `MLSDK_API_KEY` and `MLSDK_PROJECT_ID`.  To run this notebook from the command line, you can\n",
    "\n",
    "```sh\n",
    "OPENAI_API_KEY=xxx MLSDK_API_KEY=yyy MLSDK_PROJECT_ID=zzz poetry run jupyter lab examples/langchain/chatbot.ipynb\n",
    "```\n",
    "\n",
    "Lets start out with the required imports and checks for environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.callbacks import AsyncCallbackHandler\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"Error: OPENAI_API_KEY environment variable not set.\")\n",
    "    print(\"Please set your OpenAI API key:\")\n",
    "    print(\"export OPENAI_API_KEY='your-api-key-here'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if not (os.getenv(\"MLSDK_API_KEY\") and os.getenv(\"MLSDK_PROJECT_ID\") ):\n",
    "    print(\"Error: Mindlytics environment is not set up.  The latter part of this demo will not work without them\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Langchain is part of the development dependencies for this repository, so you should have no trouble importing it.  We are going \n",
    "to make a streaming chatbot, so we will need a `AsyncCallbackHandler` with a `on_llm_new_token()` method which will print the response tokens\n",
    "as they arrive from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncStreamingCallbackHandler(AsyncCallbackHandler):\n",
    "    \"\"\"Custom async callback handler for streaming output to stdout.\"\"\"\n",
    "\n",
    "    async def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        \"\"\"Called when a new token is generated.\"\"\"\n",
    "        print(token, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Now lets create a function to initialize the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"  # You can adjust this if you'd like\n",
    "\n",
    "def initialize_llm():\n",
    "    # Initialize the LLM with async streaming callback\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.7,\n",
    "        streaming=True,\n",
    "        # Add our callback handler to print tokens as they arrive\n",
    "        callbacks=[AsyncStreamingCallbackHandler()], \n",
    "    )\n",
    "\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We need a small routine to get user input from the terminal.  We can do this by executing `input()` in a thread to avoid blocking the event loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_user_input(prompt: str) -> str:\n",
    "    \"\"\"Get user input asynchronously.\"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, input, prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "And now lets create a function that controls the actual chat loop.  This loop will get the next user input, add it to chat history, invoke the LLM \n",
    "with the user input, and save the LLM response in chat history.  It will also handle the exit conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_loop(llm):\n",
    "    \"\"\"Main async chat loop that prompts for input and streams responses.\"\"\"\n",
    "    print(\"Async LangChain CLI Chat Application\")\n",
    "    print(\"Type 'quit', 'exit', or press Ctrl+C to exit\\n\")\n",
    "\n",
    "    message_history: List[BaseMessage] = []\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input asynchronously\n",
    "            user_input = await get_user_input(\"You: \")\n",
    "            user_input = user_input.strip()\n",
    "\n",
    "            # Check for exit commands\n",
    "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "                print(\"\\nGoodbye!\")\n",
    "                break\n",
    "\n",
    "            # Skip empty input\n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            # Create message and send to LLM\n",
    "            message = HumanMessage(content=user_input)\n",
    "            message_history.append(message)\n",
    "\n",
    "            print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "            # Stream the response asynchronously\n",
    "            response = await llm.ainvoke(message_history)\n",
    "            if hasattr(response, \"content\"):\n",
    "                assistant_message = AIMessage(content=response.content)\n",
    "                message_history.append(assistant_message)\n",
    "\n",
    "            print(\"\\n\")  # Add newline after streaming response\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nGoodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            print(\"Please try again.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Finally, the main loop, which initializes the LLM and runs the chat loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main async function to run the CLI application.\"\"\"\n",
    "    try:\n",
    "        # Initialize the LLM\n",
    "        llm = initialize_llm()\n",
    "\n",
    "        # Start the async chat loop\n",
    "        await chat_loop(llm)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize application: {e}\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "In a normal script, you'd run the `main()` function like this:\n",
    "\n",
    "```python\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "This is a notebook however, so instead of executing the `main()` function, lets step through some of it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = initialize_llm()\n",
    "\n",
    "# Initialize chat history\n",
    "message_history: List[BaseMessage] = []\n",
    "\n",
    "# Create a user message\n",
    "message = HumanMessage(content=\"I am looking for a good cup of coffee in Fremont CA.\")\n",
    "print(f\"User: {message.content}\")\n",
    "\n",
    "# Add it to history\n",
    "message_history.append(message)\n",
    "\n",
    "# Get a response from the LLM\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "response = await llm.ainvoke(message_history)\n",
    "\n",
    "# And add the final assistant text to message history\n",
    "assistant_message = AIMessage(content=response.content)\n",
    "message_history.append(assistant_message)\n",
    "\n",
    "print(\"\\n\")  # Add newline after streaming response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "You should see some assistant output above.  Lets do one more round, to make sure history is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = HumanMessage(content=\"Does Peet's have any donuts?\")\n",
    "print(f\"User: {message.content}\")\n",
    "message_history.append(message)\n",
    "\n",
    "print(\"Assistant: \", end=\"\", flush=True)\n",
    "response = await llm.ainvoke(message_history)\n",
    "assistant_message = AIMessage(content=response.content)\n",
    "message_history.append(assistant_message)\n",
    "print(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now we are going to add support for the `Mindlytics` service.  Lets add some imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Mindlytics API client and session, and MLEvent to capture analysis events.\n",
    "from mlsdk import Client, Session, MLEvent\n",
    "\n",
    "# import mindlytics llm callback handler for langchain\n",
    "from mlsdk.helpers.langchain import MLChatRecorderCallback\n",
    "\n",
    "# to get a unique device id for a Mindlytics session\n",
    "import uuid\n",
    "\n",
    "# This should give you a repeatable, unique id for your computer\n",
    "mac = uuid.getnode()\n",
    "device_id = f\"{mac:012x}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "To the LLM initialization code, we are going to add another callback.  This callback is a helper available on the Mindlytics SDK which will\n",
    "capture and send conversation turns to the Mindlytics service.  This callback handler requires an open Mindlytics \"session\" to be passed in its contructor, so \n",
    "our existing `initialize_llm()` needs to be modified slightly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm(session: Session):\n",
    "    # A Mindlytics session is passed in...\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.7,\n",
    "        streaming=True,\n",
    "        # Add this to enable token counts from OpenAI, which we can send to Mindlytics to capture costs\n",
    "        model_kwargs={\n",
    "            \"stream_options\": {\"include_usage\": True},\n",
    "        },\n",
    "        callbacks=[\n",
    "            AsyncStreamingCallbackHandler(),\n",
    "            MLChatRecorderCallback(session),  # Add the conversation recorder callback, passing session as an argument\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "The original `chat_loop()` function would not change in any way.  But `main()` would change to include the Mindlytics bits.\n",
    "\n",
    "For this demo, we are going to use an optional feature of the Mindlytics service.  We are going to define callbacks to capture Mindlytics analytics events (and errors) from a websocket connection established and maintained within the Mindlytics sdk.  Most applications would probably skip this part and use the Mindlytics SaaS portal to view analytics, but in this demo we want to see what Mindlytics is doing in the background.  We are going to capture events asynchroniously and look at them when the conversation is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to hold captured events and errors\n",
    "ml_events: List[MLEvent] = []\n",
    "async def on_event(event: MLEvent):\n",
    "    # Handle Mindlytics events here\n",
    "    ml_events.append(event)\n",
    "\n",
    "ml_errors: List[Exception]  = []\n",
    "async def on_error(error: Exception):\n",
    "    # Handle Mindlytics errors here\n",
    "    ml_errors.append(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Initialize the Mindlytics client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The client constructor will read MLSDK_API_KEY and MLSDK_PROJECT_ID \n",
    "ml_client = Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Now we'll create a session context.  This requires either a \"user id\" or a \"device id\".  If you know the id of your user you'd pass that.  Otherwise you should pass a device-unique id.  See the Mindlytics documentation for a detailed explination of this parameter.  Since we wish to capture real time events, we'll pass our capture callbacks here as well.  If you do not pass capture callbacks, the websocket feature will not be activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session context.\n",
    "# Clear our event and error capture arrays (in case you execute this cell again!)\n",
    "ml_events = []\n",
    "ml_errors = []\n",
    "session_context = ml_client.create_session(\n",
    "    device_id=device_id, # created earlier\n",
    "    on_event=on_event,   # capture events\n",
    "    on_error=on_error,   # capture errors\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We are going to use `session_context` in a python context.  As you know, this is a safe way to manage resources in python.  The Mindlytics sdk can take advantage of this to manage the communication with the Mindlytics service, and can automate some of the steps involved in conversation management.  You don't need to use the session in a context, but this is the easiest and safest way if you do not require more fine grain control over the Mindlytics session.\n",
    "\n",
    "Since we already have a chat history, we're going to play it again on a new LLM instance.  Here's the code to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the session\n",
    "async with session_context as session:\n",
    "    # Initialize the LLM with the open session\n",
    "    llm = initialize_llm(session)\n",
    "    # Play back our previous chat history to the LLM\n",
    "    new_message_history: List[BaseMessage] = []\n",
    "    for message in message_history:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            new_message_history.append(message)\n",
    "            print(f\"User: {message.content}\")\n",
    "            print(\"Assistant: \", end=\"\", flush=True)\n",
    "            response = await llm.ainvoke(new_message_history)\n",
    "            assistant_message = AIMessage(content=response.content)\n",
    "            new_message_history.append(assistant_message)\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Once this context is done and before its is completely exited, the Mindlytics sdk will ensure all events have been sent to the Mindlytics service, and because we are using the websockets feature, the sdk waits until it sees a final \"Session Ended\" event to ensure all events have been captured.  You might see a small delay while the sdk is waiting for the final event."
   ]
  },
  {
   "cell_type": "raw",
   "id": "29",
   "metadata": {},
   "source": [
    "Lets see what we got ... There might be a little delay until you see output ... just give it a few seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We received {len(ml_events)} events and {len(ml_errors)} errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(ml_errors) > 0:\n",
    "    print()\n",
    "    print(\"Mindlytics Errors:\")\n",
    "    for error in ml_errors:\n",
    "        print(\"------------------------------------------------\")\n",
    "        print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in ml_events:\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "You should see some events.  You should see a \"Session Started\" at the beginning and a \"Session Ended\" at the end.  Within the session there should be a \"Conversation Started\"/\"Conversation Ended\" pair, and within the conversation you will see a number of events that were captured.  Every event contains a lot of detail.  Lets take a detailed look at the \"Conversation Summary\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = next((event for event in ml_events if event.event == 'Conversation Summary'), None)\n",
    "if summary is None:\n",
    "    print(\"Cannot find the Conversation Summary event!\")\n",
    "\n",
    "for key, value in summary.properties.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "You can see some statistics about the conversation, the summary and the sentiment of the user.  Because we tracked token usage, we have a final cost calculation based on the `MODEL` you used to run the conversation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "If you want to continue this demo by examining some of the other events in detail, here is a helper function to display one or more events by name:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_events(event_name):\n",
    "    for event in ml_events:\n",
    "        if event.event == event_name:\n",
    "            print()\n",
    "            print(f\"Event: {event.event}\")\n",
    "            print(f\"  timestamp: {event.timestamp}\")\n",
    "            for prop in [\"session_id\", \"conversation_id\", \"event_id\", \"origin_event_id\"]:\n",
    "                if hasattr(event, prop):\n",
    "                    print(f\"  {prop}: {getattr(event, prop)}\")\n",
    "            if hasattr(event, \"properties\"):\n",
    "                print(\"  properties:\")\n",
    "                for key, value in event.properties.items():\n",
    "                    print(f\"    {key}: {value}\")\n",
    "            if hasattr(event, \"user_traits\"):\n",
    "                print(\"  user_traits:\")\n",
    "                for key, value in event.user_traits.items():\n",
    "                    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "And use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_events(\"Intent Informed\")\n",
    "show_events(\"Intent Closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mindlytics Python SDK",
   "language": "python",
   "name": "mindlytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
